#!/usr/bin/env node

/**
 * Test Ollama Server Connection
 * Checks if Ollama server is running and accessible
 */

const https = require('https');
const http = require('http');

// Configuration
const OLLAMA_BASE_URL = process.env.OLLAMA_API_BASE_URL || process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'vofc-engine:latest';

console.log('ðŸ” Testing Ollama Server Connection...');
console.log(`ðŸ“ Base URL: ${OLLAMA_BASE_URL}`);
console.log(`ðŸ¤– Model: ${OLLAMA_MODEL}`);
console.log('');

async function testOllamaConnection() {
  try {
    // Test 1: Check if Ollama server is running
    console.log('1ï¸âƒ£ Testing basic connectivity...');
    const healthResponse = await fetch(`${OLLAMA_BASE_URL}/api/tags`);
    
    if (!healthResponse.ok) {
      throw new Error(`HTTP ${healthResponse.status}: ${healthResponse.statusText}`);
    }
    
    const healthData = await healthResponse.json();
    console.log('âœ… Ollama server is running');
    console.log(`ðŸ“Š Available models: ${healthData.models?.length || 0}`);
    
    if (healthData.models && healthData.models.length > 0) {
      console.log('ðŸ“‹ Models found:');
      healthData.models.forEach(model => {
        console.log(`   - ${model.name} (${model.size ? Math.round(model.size / 1024 / 1024) + 'MB' : 'unknown size'})`);
      });
    }
    
    // Test 2: Check if our specific model is available
    console.log('');
    console.log('2ï¸âƒ£ Checking for target model...');
    const targetModel = healthData.models?.find(m => m.name === OLLAMA_MODEL);
    
    if (targetModel) {
      console.log(`âœ… Target model '${OLLAMA_MODEL}' is available`);
    } else {
      console.log(`âš ï¸  Target model '${OLLAMA_MODEL}' not found`);
      console.log('ðŸ’¡ Available models:');
      healthData.models?.forEach(model => {
        console.log(`   - ${model.name}`);
      });
    }
    
    // Test 3: Test a simple chat request
    console.log('');
    console.log('3ï¸âƒ£ Testing chat API...');
    const chatResponse = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: OLLAMA_MODEL,
        messages: [
          { role: 'user', content: 'Hello, are you working?' }
        ],
        stream: false
      })
    });
    
    if (!chatResponse.ok) {
      throw new Error(`Chat API failed: ${chatResponse.status} ${chatResponse.statusText}`);
    }
    
    const chatData = await chatResponse.json();
    console.log('âœ… Chat API is working');
    console.log(`ðŸ¤– Response: ${chatData.message?.content || 'No content received'}`);
    
    console.log('');
    console.log('ðŸŽ‰ Ollama server connection test PASSED!');
    console.log('âœ… Server is running and accessible');
    console.log('âœ… Chat API is functional');
    
    return true;
    
  } catch (error) {
    console.log('');
    console.log('âŒ Ollama server connection test FAILED!');
    console.log(`ðŸš¨ Error: ${error.message}`);
    console.log('');
    console.log('ðŸ”§ Troubleshooting steps:');
    console.log('1. Make sure Ollama is installed and running');
    console.log('2. Check if Ollama server is accessible at:', OLLAMA_BASE_URL);
    console.log('3. Verify the model exists: ollama list');
    console.log('4. Try pulling the model: ollama pull vofc-engine:latest');
    console.log('5. Check environment variables:');
    console.log('   - OLLAMA_API_BASE_URL or OLLAMA_BASE_URL');
    console.log('   - OLLAMA_MODEL');
    
    return false;
  }
}

// Run the test
testOllamaConnection()
  .then(success => {
    process.exit(success ? 0 : 1);
  })
  .catch(error => {
    console.error('ðŸ’¥ Test script error:', error);
    process.exit(1);
  });
