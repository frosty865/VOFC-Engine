# Ollama Configuration
# For local Ollama server:
# OLLAMA_BASE=http://localhost:11434
# For remote Ollama server (replace with actual IP):
OLLAMA_BASE=http://10.0.0.213:11434

OLLAMA_MODEL=vofc-engine:latest

# Alternative models you can use:
# OLLAMA_MODEL=llama3:latest
# OLLAMA_MODEL=mistral:latest
# OLLAMA_MODEL=nomic-embed-text:latest

# AI Service Configuration
AI_TEMPERATURE=0.2
AI_TOP_P=0.9
AI_MAX_TOKENS=2048

# Server Configuration
PORT=4000

# Supabase Configuration (if needed)
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
