# Ollama Configuration
# For local Ollama server:
OLLAMA_BASE=http://localhost:11434
# For remote Ollama server (replace with actual IP):
# OLLAMA_BASE=http://10.0.0.XXX:11434

OLLAMA_MODEL=llama3:8b-instruct

# Alternative models you can use:
# OLLAMA_MODEL=llama3.1:8b-instruct
# OLLAMA_MODEL=llama3.1:70b-instruct
# OLLAMA_MODEL=mistral:7b-instruct
# OLLAMA_MODEL=codellama:7b-instruct

# AI Service Configuration
AI_TEMPERATURE=0.2
AI_TOP_P=0.9
AI_MAX_TOKENS=2048

# Server Configuration
PORT=4000

# Supabase Configuration (if needed)
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
