"""
enhanced_parser.py
------------------
Enhanced document parser that implements document-type detection and
improved pattern extraction as recommended. This parser can handle:
- Policy guidance documents (Ready.gov, FEMA)
- VOFC table formats (SAFE Library)
- General documents with mixed content
"""

import re
import json
from pathlib import Path
from typing import Generator, Dict, List, Tuple, Optional
from datetime import datetime
import pdfplumber

class DocumentTypeDetector:
    """Detect document type and set appropriate parsing mode"""
    
    @staticmethod
    def detect_document_type(text: str, filename: str = "") -> str:
        """Detect document type based on content and filename patterns"""
        
        # Check for Ready.gov/FEMA patterns
        ready_patterns = [
            r"ready\.gov",
            r"Emergency Response Plan",
            r"Evacuation Plan", 
            r"Sheltering Plan",
            r"FEMA",
            r"Federal Emergency Management Agency"
        ]
        
        # Check for SAFE VOFC Library patterns
        safe_patterns = [
            r"SAFE VOFC Library",
            r"Category.*Vulnerability.*Options for Consideration",
            r"Vulnerability.*Options for Consideration"
        ]
        
        # Check for table-like structures
        table_patterns = [
            r"Category\s*\|\s*Vulnerability\s*\|\s*Options",
            r"^\s*[A-Z][a-z]+\s+[A-Z][a-z]+\s+[A-Z]",
            r"Table \d+",
            r"Figure \d+"
        ]
        
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        # Check Ready.gov patterns
        for pattern in ready_patterns:
            if re.search(pattern, text, re.I) or re.search(pattern, filename, re.I):
                return "policy_guidance"
        
        # Check SAFE patterns
        for pattern in safe_patterns:
            if re.search(pattern, text, re.I):
                return "vofc_table"
        
        # Check for table structures
        for pattern in table_patterns:
            if re.search(pattern, text, re.I):
                return "table_format"
        
        # Default to general parsing
        return "general"

class TextPreprocessor:
    """Clean and normalize text before parsing"""
    
    @staticmethod
    def preprocess_text(text: str) -> str:
        """Clean and normalize text content"""
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Replace common bullet points
        text = text.replace('•', '-').replace('◦', '-').replace('▪', '-')
        text = text.replace('→', '-').replace('►', '-')
        
        # Clean up PDF artifacts
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]]', ' ', text)
        
        # Fix common PDF issues
        text = re.sub(r'(\w)-\s+(\w)', r'\1\2', text)  # Fix hyphenated words
        text = re.sub(r'(\w)\s+(\w)', r'\1 \2', text)  # Fix word breaks
        
        return text.strip()

class PolicyGuidanceExtractor:
    """Extract content from policy guidance documents"""
    
    @staticmethod
    def extract_guidance_statements(text: str) -> List[Dict]:
        """Extract guidance statements and recommendations"""
        entries = []
        
        # Guidance action patterns
        guidance_patterns = [
            r"(Identify|Develop|Ensure|Train|Establish|Implement|Create|Design|Build|Construct|Install|Configure|Maintain|Monitor|Review|Update|Upgrade|Enhance|Improve|Strengthen|Secure|Protect|Safeguard)\s+[^.]{10,200}\.",
            r"(Consider|Evaluate|Assess|Analyze|Examine|Investigate|Verify|Validate|Test|Check|Audit|Inspect)\s+[^.]{10,200}\.",
            r"(Provide|Offer|Deliver|Supply|Furnish|Equip|Outfit|Prepare|Organize|Structure|Arrange|Coordinate)\s+[^.]{10,200}\.",
            r"(Adopt|Accept|Embrace|Utilize|Leverage|Employ|Apply|Use|Deploy|Activate|Enable|Facilitate)\s+[^.]{10,200}\."
        ]
        
        # Vulnerability patterns in policy context
        vulnerability_patterns = [
            r"(Lack|Missing|Absent|Insufficient|Inadequate|Deficient|Weak|Vulnerable|Exposed|Unprotected|Unsecured)\s+[^.]{10,200}\.",
            r"(Failure|Failed|Fails|Not present|No policy|No plan|No procedure|No system)\s+[^.]{10,200}\.",
            r"(Limited|Restricted|Constrained|Blocked|Prevented|Denied|Rejected)\s+[^.]{10,200}\."
        ]
        
        # Extract guidance statements (OFCs)
        for pattern in guidance_patterns:
            matches = re.finditer(pattern, text, re.I | re.MULTILINE)
            for match in matches:
                statement = match.group(0).strip()
                if len(statement.split()) > 5:  # Ensure substantial content
                    entries.append({
                        "type": "ofc",
                        "text": statement,
                        "confidence": 0.8,
                        "pattern_matched": "guidance_action",
                        "context": "policy_guidance"
                    })
        
        # Extract vulnerability statements
        for pattern in vulnerability_patterns:
            matches = re.finditer(pattern, text, re.I | re.MULTILINE)
            for match in matches:
                statement = match.group(0).strip()
                if len(statement.split()) > 5:
                    entries.append({
                        "type": "vulnerability",
                        "text": statement,
                        "confidence": 0.8,
                        "pattern_matched": "vulnerability_condition",
                        "context": "policy_guidance"
                    })
        
        return entries

class TableFormatExtractor:
    """Extract content from table-like documents"""
    
    @staticmethod
    def extract_table_entries(text: str) -> List[Dict]:
        """Extract entries from table-like structures"""
        entries = []
        
        # Split into lines and look for table patterns
        lines = text.split('\n')
        current_category = None
        current_vulnerability = None
        current_ofcs = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detect category headers
            if re.match(r'^[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*$', line) and len(line.split()) <= 3:
                if current_category and current_vulnerability and current_ofcs:
                    # Save previous entry
                    entries.append({
                        "type": "table_entry",
                        "category": current_category,
                        "vulnerability": current_vulnerability,
                        "options_for_consideration": current_ofcs,
                        "confidence": 0.9,
                        "context": "table_format"
                    })
                
                current_category = line
                current_vulnerability = None
                current_ofcs = []
            
            # Detect vulnerability statements
            elif re.search(r'(vulnerability|risk|threat|weakness|gap|deficiency)', line, re.I):
                current_vulnerability = line
            
            # Detect OFC statements
            elif re.search(r'(option|consideration|recommendation|suggestion|action|measure)', line, re.I):
                current_ofcs.append(line)
        
        # Save final entry
        if current_category and current_vulnerability and current_ofcs:
            entries.append({
                "type": "table_entry",
                "category": current_category,
                "vulnerability": current_vulnerability,
                "options_for_consideration": current_ofcs,
                "confidence": 0.9,
                "context": "table_format"
            })
        
        return entries

class GeneralExtractor:
    """Extract content from general documents"""
    
    @staticmethod
    def extract_general_content(text: str) -> List[Dict]:
        """Extract content using general patterns"""
        entries = []
        
        # Enhanced OFC patterns
        ofc_patterns = [
            r'\b(should|must|recommended|ensure|implement|encourage|establish|develop|create|install|configure|maintain|monitor|review|update|upgrade|enhance|improve|strengthen|secure|protect|safeguard)\b[^.]{10,200}\.',
            r'\b(consider|evaluate|assess|analyze|examine|investigate|verify|validate|test|check|audit|inspect)\b[^.]{10,200}\.',
            r'\b(provide|offer|deliver|supply|furnish|equip|outfit|prepare|organize|structure|arrange|coordinate)\b[^.]{10,200}\.',
            r'\b(adopt|accept|embrace|utilize|leverage|employ|apply|use|deploy|activate|enable|facilitate)\b[^.]{10,200}\.'
        ]
        
        # Enhanced vulnerability patterns
        vuln_patterns = [
            r'\b(lacks?|missing|fails?|not present|no\s+policy|insufficient|inadequate|deficient|weak|vulnerable|exposed|unprotected|unsecured|compromised|breached|violated|ignored|neglected|overlooked)\b[^.]{10,200}\.',
            r'\b(absent|unavailable|inaccessible|disabled|broken|malfunctioning|outdated|obsolete|deprecated|unsupported|unpatched|unmaintained)\b[^.]{10,200}\.',
            r'\b(limited|restricted|constrained|blocked|prevented|denied|rejected|failed|error|exception|fault|defect|flaw|weakness|gap|shortage)\b[^.]{10,200}\.',
            r'\b(unauthorized|unapproved|unverified|unvalidated|unchecked|unmonitored|uncontrolled|unmanaged|unregulated|unrestricted|unlimited)\b[^.]{10,200}\.'
        ]
        
        # Extract OFCs
        for pattern in ofc_patterns:
            matches = re.finditer(pattern, text, re.I | re.MULTILINE)
            for match in matches:
                statement = match.group(0).strip()
                if len(statement.split()) > 5:
                    entries.append({
                        "type": "ofc",
                        "text": statement,
                        "confidence": 0.7,
                        "pattern_matched": "general_ofc",
                        "context": "general"
                    })
        
        # Extract vulnerabilities
        for pattern in vuln_patterns:
            matches = re.finditer(pattern, text, re.I | re.MULTILINE)
            for match in matches:
                statement = match.group(0).strip()
                if len(statement.split()) > 5:
                    entries.append({
                        "type": "vulnerability",
                        "text": statement,
                        "confidence": 0.7,
                        "pattern_matched": "general_vulnerability",
                        "context": "general"
                    })
        
        return entries

class EnhancedParser:
    """Enhanced document parser with document-type detection"""
    
    def __init__(self):
        self.detector = DocumentTypeDetector()
        self.preprocessor = TextPreprocessor()
        self.policy_extractor = PolicyGuidanceExtractor()
        self.table_extractor = TableFormatExtractor()
        self.general_extractor = GeneralExtractor()
    
    def extract_blocks_from_pdf(self, path: str) -> Generator[str, None, None]:
        """Extract text blocks from PDF files"""
        try:
            with pdfplumber.open(path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    if not text.strip():
                        continue
                    
                    # Split into meaningful blocks
                    blocks = re.split(r'(?:\n{2,}|•|\u2022|\u2023|\u25E6|- |\d+\.\s)', text)
                    
                    for block in blocks:
                        block = self.preprocessor.preprocess_text(block)
                        if len(block.split()) > 6:
                            yield block
                            
        except Exception as e:
            print(f"Error processing PDF {path}: {e}")
            return
    
    def extract_blocks_from_text(self, path: str) -> Generator[str, None, None]:
        """Extract text blocks from text files"""
        try:
            content = Path(path).read_text(encoding="utf-8", errors="ignore")
            content = self.preprocessor.preprocess_text(content)
            
            # Split into meaningful blocks
            blocks = re.split(r'(?:\n{2,}|•|\u2022|\u2023|\u25E6|- |\d+\.\s)', content)
            
            for block in blocks:
                if len(block.split()) > 6:
                    yield block
                    
        except Exception as e:
            print(f"Error processing text file {path}: {e}")
            return
    
    def extract_citations(self, text: str) -> List[str]:
        """Extract citation references from text"""
        citations = []
        
        citation_patterns = [
            r'\[(\d+)\]',  # [1], [2], etc.
            r'\((\d+)\)',  # (1), (2), etc.
            r'Ref\.?\s*(\d+)',  # Ref 1, Ref. 1, etc.
            r'Source\s*(\d+)',  # Source 1, etc.
            r'See\s+(\d+)',  # See 1, etc.
        ]
        
        for pattern in citation_patterns:
            matches = re.findall(pattern, text, re.I)
            citations.extend(matches)
        
        return list(set(citations))
    
    def parse_document(self, path: str, source_title: str, source_metadata: Dict = None) -> List[Dict]:
        """Parse document with enhanced extraction logic"""
        records = []
        ext = Path(path).suffix.lower()
        
        # Set default metadata
        if source_metadata is None:
            source_metadata = {}
        
        # Extract source metadata
        source_type = source_metadata.get('source_type', 'unknown')
        source_url = source_metadata.get('source_url')
        author_org = source_metadata.get('author_org')
        publication_year = source_metadata.get('publication_year')
        submitted_by = source_metadata.get('submitted_by')
        content_restriction = source_metadata.get('content_restriction', 'public')
        
        print(f"Parsing document: {source_title}")
        print(f"File type: {ext}")
        print(f"Source type: {source_type}")
        
        # Extract text content
        if ext == ".pdf":
            blocks = list(self.extract_blocks_from_pdf(path))
        else:
            blocks = list(self.extract_blocks_from_text(path))
        
        if not blocks:
            print("⚠️ No content blocks extracted from document")
            return records
        
        # Combine all blocks for document type detection
        full_text = " ".join(blocks)
        document_type = self.detector.detect_document_type(full_text, Path(path).name)
        
        print(f"Detected document type: {document_type}")
        
        total_blocks = len(blocks)
        ofc_count = 0
        vuln_count = 0
        
        # Process blocks based on document type
        for block in blocks:
            # Extract citations
            citations = self.extract_citations(block)
            
            # Extract entries based on document type
            entries = []
            
            if document_type == "policy_guidance":
                entries = self.policy_extractor.extract_guidance_statements(block)
            elif document_type == "table_format":
                entries = self.table_extractor.extract_table_entries(block)
            else:
                entries = self.general_extractor.extract_general_content(block)
            
            if entries:
                # Add citations to each entry
                for entry in entries:
                    entry["citations"] = citations
                    if entry["type"] == "ofc":
                        ofc_count += 1
                    elif entry["type"] == "vulnerability":
                        vuln_count += 1
                
                records.append({
                    "source_title": source_title,
                    "source_type": source_type,
                    "source_url": source_url,
                    "author_org": author_org,
                    "publication_year": publication_year,
                    "submitted_by": submitted_by,
                    "content_restriction": content_restriction,
                    "source_file": path,
                    "document_type": document_type,
                    "content": entries,
                    "extraction_timestamp": datetime.now().isoformat(),
                    "block_text": block[:200] + "..." if len(block) > 200 else block
                })
        
        print(f"Extraction complete:")
        print(f"  - Document type: {document_type}")
        print(f"  - Total blocks processed: {total_blocks}")
        print(f"  - OFCs found: {ofc_count}")
        print(f"  - Vulnerabilities found: {vuln_count}")
        print(f"  - Records created: {len(records)}")
        
        return records

def parse_document_enhanced(path: str, source_title: str, source_metadata: Dict = None) -> List[Dict]:
    """Enhanced document parsing function"""
    parser = EnhancedParser()
    return parser.parse_document(path, source_title, source_metadata)

if __name__ == "__main__":
    # Example usage
    import sys
    
    if len(sys.argv) > 1:
        sample_path = sys.argv[1]
        sample_title = sys.argv[2] if len(sys.argv) > 2 else "Sample Document"
        
        if Path(sample_path).exists():
            data = parse_document_enhanced(sample_path, sample_title)
            
            # Save results
            output_file = Path("parsed_enhanced.json")
            output_file.write_text(json.dumps(data, indent=2, ensure_ascii=False))
            
            print(f"Results saved to {output_file}")
            print(f"Extracted {len(data)} content blocks")
        else:
            print(f"File not found: {sample_path}")
    else:
        print("Usage: python enhanced_parser.py <file_path> [document_title]")
        print("Example: python enhanced_parser.py sample.pdf 'FEMA Reference Manual 426'")
